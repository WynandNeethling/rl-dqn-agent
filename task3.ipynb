{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import *\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputSize, numActions, hiddenLayerSize=(512, 256)):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, hiddenLayerSize[0])\n",
    "        self.fc2 = nn.Linear(hiddenLayerSize[0], hiddenLayerSize[1])\n",
    "        self.fc3 = nn.Linear(hiddenLayerSize[1], numActions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def extractObjectInformation2(observation):\n",
    "    (rows, cols, x) = observation.shape\n",
    "    tmp = np.reshape(observation,[rows*cols*x,1], 'F')[0:rows*cols]\n",
    "    return np.reshape(tmp, [rows,cols],'C')\n",
    "\n",
    "def normalize(observation, max_value):\n",
    "    return np.array(observation)/max_value\n",
    "\n",
    "def flatten(observation):\n",
    "    return torch.from_numpy(np.array(observation).flatten()).float().unsqueeze(0)\n",
    "\n",
    "def preprocess(observation):\n",
    "    return flatten(normalize(extractObjectInformation2(observation), 10.0))\n",
    "\n",
    "def select_action(state):\n",
    "    sample = random.random()\n",
    "    eps_threshold = stop_epsilon+(start_epsilon-stop_epsilon)*math.exp(-1. * steps_done / decay_rate)\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].unsqueeze(0)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(numActions)]], device=device, dtype=torch.long)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('currentState', 'action', 'nextState', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "memory = ReplayMemory(memorySize)\n",
    "\n",
    "def optimize_model():\n",
    "    # check if the replay memory has stored enough experience\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample mini-batch\n",
    "    experience = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*experience))\n",
    "\n",
    "    # Calculate action-values using policy network\n",
    "    state_batch = torch.cat(batch.currentState)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    # Calculate the action-values for each state in the batch, and \n",
    "    # then gather the Q-value for the action associated with a specific state\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calculate TD-targets using target network\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    non_final_next_states = torch.cat([s for s in batch.nextState if s is not None])\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.nextState)), device=device, dtype=torch.bool)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    TDtargets = (next_state_values * gamma) + reward_batch\n",
    "    TDerrors = TDtargets.unsqueeze(1) - state_action_values\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, TDtargets.unsqueeze(1))\n",
    "\n",
    "    # Make gradient descrent step and update policy network\n",
    "    optimizer.zero_grad() #optimizer ?\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS \n",
    "numActions = 3\n",
    "inputSize = 49\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "alpha = 0.0002\n",
    "episodes = 5000            \n",
    "batch_size = 128\n",
    "target_update = 20000\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.90        \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "start_epsilon = 1.0      \n",
    "stop_epsilon = 0.01       \n",
    "decay_rate = 20000       \n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size\n",
    "memorySize = 500000\n",
    "\n",
    "### TESTING HYPERPARAMETERS\n",
    "evalEpisodes = 1000\n",
    "\n",
    "# Change this to 'False' if you only want to evaluate a previously trained agent\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the weights of the policy network and target networks  \n",
    "hiddenLayerSize = (128,128)\n",
    "policy_net = DQN(inputSize, numActions, hiddenLayerSize)\n",
    "target_net = DQN(inputSize, numActions, hiddenLayerSize)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "## Initialize the environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50\n",
    "max_steps = env.max_steps\n",
    "steps_done = 0\n",
    "\n",
    "print('Start training...')\n",
    "for e in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = preprocess(obs)\n",
    "    \n",
    "    for i in range(0, max_steps):\n",
    "        action = select_action(state)\n",
    "        a = action.item()\n",
    "        steps_done += 1\n",
    "        \n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        reward = torch.tensor([reward], device = device)\n",
    "   \n",
    "        nextState = preprocess(obs)\n",
    "\n",
    "        if (done or truncated):\n",
    "            nextState = None\n",
    "        \n",
    "        memory.push(state, action, nextState, reward)\n",
    "        \n",
    "        currentState = nextState\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if steps_done % target_update == 0:\n",
    "            print(\"updating network\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())  \n",
    "\n",
    "        \n",
    "        # Episode finished when done or truncated is true\n",
    "        if (done or truncated):\n",
    "            # Record the reward and total training steps taken\n",
    "            if (done):\n",
    "                # if agent reached its goal successfully\n",
    "                print('Finished episode successfully taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "            else:\n",
    "                # agent failed to reach its goal successfully \n",
    "                print('Truncated episode taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "            break\n",
    "            \n",
    "        \n",
    "print('Done training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation loop\n",
    "finishCounter = 0.0\n",
    "totalSteps = 0.0\n",
    "totalReward = 0.0\n",
    "\n",
    "steps_done = 1000000\n",
    "stop_epsilon = 0.0\n",
    "evalEpisodes = 2\n",
    "\n",
    "for e in range(evalEpisodes):\n",
    "    # Initialize the environment and state\n",
    "    currentObs, _ = env.reset()\n",
    "    currentState = preprocess(currentObs)\n",
    "   \n",
    "    # the main RL loop\n",
    "    for i in range(0, env.max_steps):\n",
    "        # Select and perform an action\n",
    "        action = select_action(currentState)\n",
    "        a = action.item()\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        \n",
    "        if (done or truncated):\n",
    "            # Observe new state\n",
    "            nextState = None\n",
    "        else:\n",
    "            nextState = preprocess(obs)\n",
    "\n",
    "        if (done or truncated):\n",
    "            totalReward += reward\n",
    "            totalSteps += env.step_count\n",
    "            if (done):\n",
    "                print('Finished evaluation episode %d with reward %f,  %d steps, reaching goal ' % (e, reward, env.step_count))\n",
    "                finishCounter += 1\n",
    "            if (truncated):\n",
    "                print('Failed evaluation episode %d with reward %f, %d steps' % (e,reward, env.step_count))\n",
    "            break\n",
    "        \n",
    "        # Move to the next state\n",
    "        currentState = nextState\n",
    "\n",
    "# Print a summary of the evaluation results\n",
    "print('Completion rate %.2f with average reward %0.4f and average steps %0.2f' % (finishCounter/evalEpisodes, totalReward/evalEpisodes,  totalSteps/evalEpisodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
